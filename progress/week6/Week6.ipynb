{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 6\n",
    "\n",
    "## 1. Progress\n",
    "We trained the insertion, deletion and indel models using the training data that was available on FigShare. This\n",
    "resulted in six models, namely 2 regularizer types * 3 models. It wasn't extremely clear in their paper but in their\n",
    "supplementary they mention that for their final model they simply pick the best performing model whether it be L2 or L1\n",
    "at the specific value it perform best at.\n",
    "\n",
    "We also rewrote some of the code that was used to train the models to allow for better jupyter widgets and make it more\n",
    "clear what the difference is between the different types of models when it comes to training.\n",
    "\n",
    "### Training the models and reproducing regularization strength images\n",
    "![Insertion MSE](../../assets/InsertionModelMSE.png)\n",
    "Since there is a slight dip in l1 around 10e-3 we opted to go for the l1 model\n",
    "\n",
    "![Deletion MSE](../../assets/DeletionModelMSE.png)\n",
    "Since there is a clear dip in l1 around 10e-3 we opted to go for the l1 model\n",
    "\n",
    "![Indel MSE](../../assets/IndelModelMSE.png)\n",
    "Since there is a slightly lower dip in L2 after 10e-3 we opted to go for the l2 model\n",
    "\n",
    "### Running our trained model on the test set and generating the MSE count figure (6.B)\n",
    "After getting these three models trained, we wrote a script that ran the test set over the models. This then allowed us\n",
    "to plot the mse count estimating the bin size that was used in Figure 6.B:\n",
    "```python\n",
    "errors = run_test_set(file, file_dir)\n",
    "x_range_label = np.arange(0, 1.6, .2)\n",
    "x_range_label_str = list(map(lambda x: str(round(x, 1)), x_range_label))\n",
    "x_range = (10 ** -3) * x_range_label\n",
    "\n",
    "fig, (ax) = plt.subplots(nrows=1)\n",
    "ax.hist(errors, bins=6 * len(x_range_label), color='aqua', alpha=.4, edgecolor='grey')\n",
    "ax.xaxis.set_ticks(x_range)\n",
    "ax.xaxis.set_ticklabels(x_range_label_str)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.set_xlim(x_range[0] - x_range[1] / 9, x_range[-1])\n",
    "plt.xlabel(\"MSE 10e-3\")\n",
    "plt.ylabel(\"Count\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see from running this, the plot generated is not much different from the 6.B plot.\n",
    "<p float=\"left\">\n",
    "  <img src=\"../../assets/6BMseCount.png\" style=\"float:left;\"/>\n",
    "  <img src=\"../../assets/MseCount.png\" style=\"float:left;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To generate such an image we took a bit out of the Predictor.py and used the 6.A figure as a reference\n",
    "\n",
    "```python\n",
    "input_indel = onehotencoder(seq)\n",
    "input_ins = onehotencoder(seq[-6:])\n",
    "input_del = x_in\n",
    "\n",
    "dratio, insratio = softmax(np.dot(input_indel, w1) + b1)\n",
    "ds = softmax(np.dot(input_del, w2) + b2)\n",
    "ins = softmax(np.dot(input_ins, w3) + b3)\n",
    "y_hat = np.concatenate((ds * dratio, ins * insratio), axis=None)\n",
    "errors.append(mse(y_hat, y_out))\n",
    "```\n",
    "\n",
    "### Issues with profiles and basepairs\n",
    "We quickly noticed that to create a model that was similar to the Predictor.py model we would need to 65p sequence and not\n",
    "the 20bp sequence that was found in the target set. We then had contact with the TA's and they helped us out by stating that\n",
    "we were supposed to be able to generate our own targets + features + lindel profiles based on the algient file and the rep matrices.\n",
    "\n",
    "When this was said to us, it made sense because it was hard to see the challenge past running the models on the training/test set that\n",
    "was provided on the figshare. From that moment we split the tasks:\n",
    "\n",
    "1. One group needs to figure out how to use the matrices, feature_index_all and algient files to construct the binary features\n",
    "required to train the model. The hard part here is generating the microhomology binary features since those of the sequence are just\n",
    "one-hot-encoded based on the 20pb target and are quite easy to get by.\n",
    "\n",
    "\n",
    "2. The other group needs to figure out how to use the same data to generate the lindel profiles. The progress here is already\n",
    "good. We now understand how the matrix files correspond to the labels and that we can produce a total frequency for each target and event\n",
    "and normalize this to get the lindel profile.\n",
    "\n",
    "## 2. Plan for thursday/this week\n",
    "\n",
    "1. We want to be able to generate our own training/test set files based on the files mentioned above\n",
    "    a. If this gets done, we want to train this on our model and see how it performs in comparison to the provided sets\n",
    "\n",
    "2. We aim to understand how redundant deletion events can be combined in order to end up with ~450 classes as opposed to\n",
    "the current 558\n",
    "\n",
    "## 3. Questions for thursday\n",
    "\n",
    "1. We were able to train the Lindel model on the Lindel dataset in a timeframe if +- ~1hour. Assuming that generation of this data\n",
    "does not take longer (which so far seems to be the case), do we need to train a model on the combination of forecast/lindel data?\n",
    "Why else would we need the google credits?\n",
    "\n",
    "\n",
    "2. If we reproduce the model to this extent (test/training set generation from matrices/algient files, training models based on that\n",
    "and feeding weights to the Predictor.py in order to make it functional), what is the next step? We will technically have reproduced the results of\n",
    "the paper as far as we understand it?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}